{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92fb7df4",
   "metadata": {},
   "source": [
    "<h2> Imports, loading event-log function and cleaning pipeline </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188cc927",
   "metadata": {},
   "source": [
    "<h4> TODO List </h4>\n",
    "<ul>\n",
    "    <li> Refactor some code, mainly abstrahize some training and testing logic </li>\n",
    "    <li> Grid search to find best parameters for each feature and model </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "819e851f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pm4py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import statistics\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a09798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dataset from file path\n",
    "def import_xes(file_path):\n",
    "    log = pm4py.read_xes(file_path)\n",
    "    return pm4py.convert_to_dataframe(log)\n",
    "\n",
    "# Cleaning dataset: removing unnecessary columns, shifting to resource focus\n",
    "def clean_dataset(df):\n",
    "    df_final = df[['case:concept:name', 'concept:name', 'org:resource', 'time:timestamp']]\n",
    "    return df_final.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "def prefix_extraction(df, min_len=1, max_len=None):\n",
    "    \"\"\"\n",
    "    Extract prefizes of activity sequences for each resource.\n",
    "    Each resourece has a timeline, this creates prefixes of what \n",
    "    that resource has done up until each point in time.\n",
    "    \"\"\"\n",
    "    resource_traces = df.groupby('org:resource')['concept:name'].apply(list)\n",
    "    all_prefix_rows = []\n",
    "\n",
    "    for resource, seq in resource_traces.items():\n",
    "        if max_len is None:\n",
    "            max_prefix_len = len(seq)\n",
    "        else:\n",
    "            max_prefix_len = min(max_len, len(seq))\n",
    "\n",
    "        for k in range(min_len, max_prefix_len + 1):\n",
    "            prefix = seq[:k]\n",
    "            all_prefix_rows.append({\n",
    "                'resource' : resource,\n",
    "                'prefix_length' : k,\n",
    "                'prefix' : prefix\n",
    "            })\n",
    "    return pd.DataFrame(all_prefix_rows)\n",
    "\n",
    "def apply_bucketing(prefix_df):\n",
    "    \"\"\"\n",
    "    Takes the prefix datafram with columns:\n",
    "        1. resource that performs\n",
    "        2. prefix_length\n",
    "        3. prefix (list of activities)\n",
    "    \n",
    "    To this we add:\n",
    "        1. last_activity\n",
    "        2. bucket_key\n",
    "        3. bucket_id\n",
    "\n",
    "    In essence this bucket buckets prefixes that have the \n",
    "    same lenght AND share the same final activity. \n",
    "\n",
    "    The tuples are then also converted to hashes to make \n",
    "    sure any model can work fine with our buckets, even\n",
    "    if they require numerical bucket IDs\n",
    "    \"\"\"\n",
    "\n",
    "    # extract last activity for each prefix\n",
    "    prefix_df['last_activity'] = prefix_df['prefix'].apply(\n",
    "        lambda p: p[-1] if isinstance(p,list) and len(p) > 0 else None\n",
    "    )\n",
    "\n",
    "    # creating tuple keys used for bucketing --> defines semantic bucket\n",
    "    prefix_df['bucket_key'] = prefix_df.apply(\n",
    "        lambda row: (row['prefix_length'], row['last_activity']),\n",
    "        axis = 1\n",
    "    )\n",
    "\n",
    "    # creating stable bucket ID based on the key\n",
    "    prefix_df['bucket_id'] = prefix_df['bucket_key'].apply(\n",
    "        lambda k: abs(hash(k)) % 10_000_000\n",
    "    )\n",
    "\n",
    "    return prefix_df\n",
    "\n",
    "\n",
    "def last_state_encoding(df):\n",
    "    # Applying last-state encoding to prevent data leakage\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    df_encoded['last_activity'] = df_encoded['prefix'].apply(\n",
    "        lambda p: p[-1] if p else None\n",
    "    )\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "def process_dataset(file_path, min_len=1, max_len=None):\n",
    "    df = import_xes(file_path)\n",
    "    df_clean = clean_dataset(df)\n",
    "    df_prefixes = prefix_extraction(df_clean, min_len=min_len, max_len=max_len)\n",
    "    df_bucketed = apply_bucketing(df_prefixes)\n",
    "    df_encoded = last_state_encoding(df_bucketed)\n",
    "\n",
    "    return df_encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07556d6f",
   "metadata": {},
   "source": [
    "<h1> Loading event-logs and transforming</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9d87f",
   "metadata": {},
   "source": [
    "<h4> Loading datasets </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e27c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gijskoppenberg/Documents/UU Jaar 4/OZP/Resource-Centric-NAP/.venv/lib/python3.14/site-packages/pm4py/utils.py:992: UserWarning: Install the optional requirement `rustxes` to import/export files faster.\n",
      "  warnings.warn(\"Install the optional requirement `rustxes` to import/export files faster.\")\n",
      "/Users/gijskoppenberg/Documents/UU Jaar 4/OZP/Resource-Centric-NAP/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "parsing log, completed traces :: 100%|██████████| 7554/7554 [00:01<00:00, 3874.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sucessfully loaded 2013 dataset\n"
     ]
    }
   ],
   "source": [
    "df_2013 = process_dataset(\"datasets/BPI_Challenge_2013_incidents.xes\")\n",
    "print(\"Sucessfully loaded 2013 dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9684e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing log, completed traces :: 100%|██████████| 31509/31509 [00:22<00:00, 1387.65it/s]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_2017 = process_dataset(\"datasets/BPI_Challenge_2017.xes\")\n",
    "print(\"Sucessfully loaded 2017 dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eb4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2018 = process_dataset(\"datasets/BPI_Challenge_2018.xes\")\n",
    "print(\"Sucessfully loaded 2018 dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e2241",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = process_dataset(\"datasets/BPI_Challenge_2019.xes\")\n",
    "print(\"Succesfully loaded, bucket, and encoded all datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551418da",
   "metadata": {},
   "source": [
    "<h1>One-Hot Encoding the event-logs</h1>\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b687e",
   "metadata": {},
   "source": [
    "<h4> Apply One-Hot encoding function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb7b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_one_hot_encoding(df, columns=['last_activity', 'resource']):\n",
    "    df_encoded = df.copy()\n",
    "\n",
    "    for col in columns:\n",
    "        if col in df_encoded.columns:\n",
    "            dummies = pd.get_dummies(df_encoded[col], prefix=col)\n",
    "            df_encoded = pd.concat([df_encoded.drop(col, axis=1), dummies], axis=1)\n",
    "    return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cd6c4b",
   "metadata": {},
   "source": [
    "<h4> OHE the BPIC 2013 event-log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4914b184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   prefix_length                                             prefix  \\\n",
      "0              1                                         [Accepted]   \n",
      "1              2                               [Accepted, Accepted]   \n",
      "2              3                     [Accepted, Accepted, Accepted]   \n",
      "3              4           [Accepted, Accepted, Accepted, Accepted]   \n",
      "4              5  [Accepted, Accepted, Accepted, Accepted, Accep...   \n",
      "\n",
      "      bucket_key  bucket_id  last_activity_Accepted  last_activity_Completed  \\\n",
      "0  (1, Accepted)    3013051                    True                    False   \n",
      "1  (2, Accepted)    1375001                    True                    False   \n",
      "2  (3, Accepted)    8870692                    True                    False   \n",
      "3  (4, Accepted)     660535                    True                    False   \n",
      "4  (5, Accepted)    6457247                    True                    False   \n",
      "\n",
      "   last_activity_Queued  last_activity_Unmatched  resource_-  resource_Aaron  \\\n",
      "0                 False                    False        True           False   \n",
      "1                 False                    False        True           False   \n",
      "2                 False                    False        True           False   \n",
      "3                 False                    False        True           False   \n",
      "4                 False                    False        True           False   \n",
      "\n",
      "   ...  resource_Yvan  resource_Zacharias  resource_Zachary  \\\n",
      "0  ...          False               False             False   \n",
      "1  ...          False               False             False   \n",
      "2  ...          False               False             False   \n",
      "3  ...          False               False             False   \n",
      "4  ...          False               False             False   \n",
      "\n",
      "   resource_Zbigniew  resource_Zelda  resource_Zoi  resource_yoshiyuki  \\\n",
      "0              False           False         False               False   \n",
      "1              False           False         False               False   \n",
      "2              False           False         False               False   \n",
      "3              False           False         False               False   \n",
      "4              False           False         False               False   \n",
      "\n",
      "   resource_Åke  resource_Åsa  resource_Åse  \n",
      "0         False         False         False  \n",
      "1         False         False         False  \n",
      "2         False         False         False  \n",
      "3         False         False         False  \n",
      "4         False         False         False  \n",
      "\n",
      "[5 rows x 1448 columns]\n"
     ]
    }
   ],
   "source": [
    "df_2013_onehot = apply_one_hot_encoding(df_2013)\n",
    "\n",
    "print(df_2013_onehot.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b88275",
   "metadata": {},
   "source": [
    "<h4> OHE the BPIC 2017 event-log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5928c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in final dataframe 2017:\n",
      "['case:concept:name', 'time:timestamp', 'next_activity', 'activity_A_Accepted', 'activity_A_Cancelled', 'activity_A_Complete', 'activity_A_Concept', 'activity_A_Create Application', 'activity_A_Denied', 'activity_A_Incomplete', 'activity_A_Pending', 'activity_A_Submitted', 'activity_A_Validating', 'activity_O_Accepted', 'activity_O_Cancelled', 'activity_O_Create Offer', 'activity_O_Created', 'activity_O_Refused', 'activity_O_Returned', 'activity_O_Sent (mail and online)', 'activity_O_Sent (online only)', 'activity_W_Assess potential fraud', 'activity_W_Call after offers', 'activity_W_Call incomplete files', 'activity_W_Complete application', 'activity_W_Handle leads', 'activity_W_Personal Loan collection', 'activity_W_Shortened completion ', 'activity_W_Validate application', 'resource_User_1', 'resource_User_10', 'resource_User_100', 'resource_User_101', 'resource_User_102', 'resource_User_103', 'resource_User_104', 'resource_User_105', 'resource_User_106', 'resource_User_107', 'resource_User_108', 'resource_User_109', 'resource_User_11', 'resource_User_110', 'resource_User_111', 'resource_User_112', 'resource_User_113', 'resource_User_114', 'resource_User_115', 'resource_User_116', 'resource_User_117', 'resource_User_118', 'resource_User_119', 'resource_User_12', 'resource_User_120', 'resource_User_121', 'resource_User_122', 'resource_User_123', 'resource_User_124', 'resource_User_125', 'resource_User_126', 'resource_User_127', 'resource_User_128', 'resource_User_129', 'resource_User_13', 'resource_User_130', 'resource_User_131', 'resource_User_132', 'resource_User_133', 'resource_User_134', 'resource_User_135', 'resource_User_136', 'resource_User_137', 'resource_User_138', 'resource_User_139', 'resource_User_14', 'resource_User_140', 'resource_User_141', 'resource_User_142', 'resource_User_143', 'resource_User_144', 'resource_User_145', 'resource_User_146', 'resource_User_147', 'resource_User_148', 'resource_User_149', 'resource_User_15', 'resource_User_16', 'resource_User_17', 'resource_User_18', 'resource_User_19', 'resource_User_2', 'resource_User_20', 'resource_User_21', 'resource_User_22', 'resource_User_23', 'resource_User_24', 'resource_User_25', 'resource_User_26', 'resource_User_27', 'resource_User_28', 'resource_User_29', 'resource_User_3', 'resource_User_30', 'resource_User_31', 'resource_User_32', 'resource_User_33', 'resource_User_34', 'resource_User_35', 'resource_User_36', 'resource_User_37', 'resource_User_38', 'resource_User_39', 'resource_User_4', 'resource_User_40', 'resource_User_41', 'resource_User_42', 'resource_User_43', 'resource_User_44', 'resource_User_45', 'resource_User_46', 'resource_User_47', 'resource_User_48', 'resource_User_49', 'resource_User_5', 'resource_User_50', 'resource_User_51', 'resource_User_52', 'resource_User_53', 'resource_User_54', 'resource_User_55', 'resource_User_56', 'resource_User_57', 'resource_User_58', 'resource_User_59', 'resource_User_6', 'resource_User_60', 'resource_User_61', 'resource_User_62', 'resource_User_63', 'resource_User_64', 'resource_User_65', 'resource_User_66', 'resource_User_67', 'resource_User_68', 'resource_User_69', 'resource_User_7', 'resource_User_70', 'resource_User_71', 'resource_User_72', 'resource_User_73', 'resource_User_74', 'resource_User_75', 'resource_User_76', 'resource_User_77', 'resource_User_78', 'resource_User_79', 'resource_User_8', 'resource_User_80', 'resource_User_81', 'resource_User_82', 'resource_User_83', 'resource_User_84', 'resource_User_85', 'resource_User_86', 'resource_User_87', 'resource_User_88', 'resource_User_89', 'resource_User_9', 'resource_User_90', 'resource_User_91', 'resource_User_92', 'resource_User_93', 'resource_User_94', 'resource_User_95', 'resource_User_96', 'resource_User_97', 'resource_User_98', 'resource_User_99']\n"
     ]
    }
   ],
   "source": [
    "df_2017['next_activity'] = df_2017.groupby('org:resource')['concept:name'].shift(-1)\n",
    "df_2017 = df_2017.dropna(subset=['next_activity'])\n",
    "\n",
    "# Encoding the 'concept:name' column, as these are the activities\n",
    "column_to_encode = 'concept:name'\n",
    "\n",
    "# OHE BPIC 2017 using pandas' get_dummies\n",
    "df_2017_OHE = pd.get_dummies(df_2017, columns=[column_to_encode], prefix='activity')\n",
    "df_2017_OHE = pd.get_dummies(df_2017_OHE, columns=['org:resource'], prefix='resource')\n",
    "\n",
    "print(\"Columns in final dataframe 2017:\")\n",
    "print(df_2017_OHE.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116a7a2",
   "metadata": {},
   "source": [
    "<h4> OHE the BPIC 2018 event-log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4bfffb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in final dataframe 2018:\n",
      "['case:concept:name', 'time:timestamp', 'next_activity', 'activity_abort external', 'activity_abort payment', 'activity_approve', 'activity_begin admissibility check', 'activity_begin editing', 'activity_begin editing from refused', 'activity_begin payment', 'activity_begin preparations', 'activity_calculate', 'activity_calculate protocol', 'activity_cancel offline', 'activity_change department', 'activity_check', 'activity_check admissibility', 'activity_clear', 'activity_correction GFM17', 'activity_create', 'activity_decide', 'activity_discard', 'activity_finish editing', 'activity_finish payment', 'activity_finish pre-check', 'activity_finish preparations', 'activity_initialize', 'activity_insert document', 'activity_mail income', 'activity_mail valid', 'activity_performed', 'activity_performed offline', 'activity_plan', 'activity_prepare external', 'activity_prepare offline', 'activity_refuse', 'activity_remove document', 'activity_restart editing', 'activity_revoke approval', 'activity_revoke decision', 'activity_revoke withdrawal', 'activity_save', 'activity_take original document', 'activity_withdraw', 'resource_00037f', 'resource_0087cf', 'resource_019209', 'resource_023bb9', 'resource_03b214', 'resource_059abc', 'resource_07361d', 'resource_08e484', 'resource_0;n/a', 'resource_0d884f', 'resource_0fe39b', 'resource_13c937', 'resource_155add', 'resource_167893', 'resource_1bd3b5', 'resource_1c1894', 'resource_2044b6', 'resource_21612d', 'resource_237892', 'resource_27cc37', 'resource_28b6bf', 'resource_2ac4ac', 'resource_2b8616', 'resource_2baab0', 'resource_2bf205', 'resource_2c546f', 'resource_2ca0ae', 'resource_2dc625', 'resource_313338', 'resource_346f05', 'resource_352a49', 'resource_354865', 'resource_36c75c', 'resource_39be3e', 'resource_3d952e', 'resource_40178c', 'resource_425ee3', 'resource_4298e3', 'resource_439089', 'resource_44798d', 'resource_465290', 'resource_46e3af', 'resource_478c4f', 'resource_483029', 'resource_4af6fb', 'resource_4b9a7f', 'resource_4e9bc2', 'resource_4fac6d', 'resource_51d239', 'resource_520882', 'resource_556da9', 'resource_5a21a5', 'resource_5bf77a', 'resource_5c1b3c', 'resource_5d8173', 'resource_5d9e09', 'resource_5dd4ec', 'resource_5e018b', 'resource_60b0b8', 'resource_65bf34', 'resource_6ad4a6', 'resource_6c720c', 'resource_6d6ae5', 'resource_6f6a4e', 'resource_6fde6b', 'resource_7078c7', 'resource_71ffe8', 'resource_727350', 'resource_75992a', 'resource_76f30e', 'resource_7850e8', 'resource_7886b1', 'resource_79367e', 'resource_796771', 'resource_7b9b55', 'resource_7d03a0', 'resource_7d12ba', 'resource_7d4a25', 'resource_7fb8a5', 'resource_815d19', 'resource_822fb7', 'resource_82518f', 'resource_83c7b7', 'resource_8a1fba', 'resource_8aab8f', 'resource_8beb64', 'resource_8c9a01', 'resource_8d8538', 'resource_91bca0', 'resource_9765da', 'resource_97d224', 'resource_99e048', 'resource_9aaf68', 'resource_9e088c', 'resource_9e337f', 'resource_DP-R', 'resource_DP-Z', 'resource_Document processing automaton', 'resource_Inspection automaton', 'resource_Inspection service', 'resource_Notification automaton', 'resource_Parcel automaton', 'resource_Processing automaton', 'resource_Reference alignment processor', 'resource_Remote inspection export', 'resource_Remote inspection import', 'resource_a21e1b', 'resource_a5ae3f', 'resource_a5c3dd', 'resource_abe845', 'resource_ad25fc', 'resource_aea7c8', 'resource_af1e1e', 'resource_b2b786', 'resource_b33ed5', 'resource_b4755f', 'resource_b90293', 'resource_ba978b', 'resource_bbd307', 'resource_bc063d', 'resource_bfa0ec', 'resource_c0aa4a', 'resource_c39cb2', 'resource_c4f075', 'resource_c522f8', 'resource_c70c37', 'resource_c7c478', 'resource_c966f4', 'resource_c98a4a', 'resource_d0f451', 'resource_d114c8', 'resource_d4758f', 'resource_d4d37d', 'resource_d85681', 'resource_d8639c', 'resource_dd04e2', 'resource_dd12cf', 'resource_dde669', 'resource_dee71b', 'resource_dfd1d4', 'resource_e25f20', 'resource_e99397', 'resource_ea52aa', 'resource_eb38eb', 'resource_eb7015', 'resource_ee262f', 'resource_ee28de', 'resource_ef80e3', 'resource_f01c35', 'resource_f3e81e', 'resource_f48280', 'resource_f6846a', 'resource_f6f7f1', 'resource_f7f7b6', 'resource_f8da29', 'resource_f9fe07', 'resource_fa6437', 'resource_fa68d2', 'resource_fb5fa8', 'resource_fc6177', 'resource_fcb55b', 'resource_ffb56b', 'resource_parcel correction automaton', 'resource_scheduler']\n"
     ]
    }
   ],
   "source": [
    "df_2018['next_activity'] = df_2018.groupby('org:resource')['concept:name'].shift(-1)\n",
    "df_2018 = df_2018.dropna(subset=['next_activity'])\n",
    "\n",
    "# Encoding the 'concept:name' column, as these are the activities\n",
    "column_to_encode = 'concept:name'\n",
    "\n",
    "# OHE BPIC 2018 using pandas' get_dummies\n",
    "df_2018_OHE = pd.get_dummies(df_2018, columns=[column_to_encode], prefix='activity')\n",
    "df_2018_OHE = pd.get_dummies(df_2018_OHE, columns=['org:resource'], prefix='resource')\n",
    "\n",
    "print(\"Columns in final dataframe 2018:\")\n",
    "print(df_2018_OHE.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1f953",
   "metadata": {},
   "source": [
    "<h4> OHE the BPIC 2019 event-log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9efe2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in final dataframe 2019:\n",
      "['case:concept:name', 'time:timestamp', 'next_activity', 'activity_Block Purchase Order Item', 'activity_Cancel Goods Receipt', 'activity_Cancel Invoice Receipt', 'activity_Cancel Subsequent Invoice', 'activity_Change Approval for Purchase Order', 'activity_Change Currency', 'activity_Change Delivery Indicator', 'activity_Change Final Invoice Indicator', 'activity_Change Price', 'activity_Change Quantity', 'activity_Change Rejection Indicator', 'activity_Change Storage Location', 'activity_Change payment term', 'activity_Clear Invoice', 'activity_Create Purchase Order Item', 'activity_Create Purchase Requisition Item', 'activity_Delete Purchase Order Item', 'activity_Reactivate Purchase Order Item', 'activity_Receive Order Confirmation', 'activity_Record Goods Receipt', 'activity_Record Invoice Receipt', 'activity_Record Service Entry Sheet', 'activity_Record Subsequent Invoice', 'activity_Release Purchase Order', 'activity_Release Purchase Requisition', 'activity_Remove Payment Block', 'activity_SRM: Awaiting Approval', 'activity_SRM: Change was Transmitted', 'activity_SRM: Complete', 'activity_SRM: Created', 'activity_SRM: Deleted', 'activity_SRM: Document Completed', 'activity_SRM: Held', 'activity_SRM: In Transfer to Execution Syst.', 'activity_SRM: Incomplete', 'activity_SRM: Ordered', 'activity_SRM: Transaction Completed', 'activity_SRM: Transfer Failed (E.Sys.)', 'activity_Set Payment Block', 'activity_Update Order Confirmation', 'activity_Vendor creates debit memo', 'activity_Vendor creates invoice', 'resource_NONE', 'resource_batch_00', 'resource_batch_01', 'resource_batch_02', 'resource_batch_03', 'resource_batch_04', 'resource_batch_05', 'resource_batch_06', 'resource_batch_07', 'resource_batch_08', 'resource_batch_09', 'resource_batch_10', 'resource_batch_11', 'resource_batch_12', 'resource_batch_13', 'resource_batch_14', 'resource_batch_15', 'resource_batch_16', 'resource_batch_17', 'resource_batch_18', 'resource_batch_19', 'resource_user_000', 'resource_user_001', 'resource_user_002', 'resource_user_003', 'resource_user_004', 'resource_user_005', 'resource_user_006', 'resource_user_007', 'resource_user_008', 'resource_user_009', 'resource_user_010', 'resource_user_011', 'resource_user_012', 'resource_user_013', 'resource_user_014', 'resource_user_015', 'resource_user_016', 'resource_user_017', 'resource_user_018', 'resource_user_019', 'resource_user_020', 'resource_user_021', 'resource_user_022', 'resource_user_023', 'resource_user_024', 'resource_user_025', 'resource_user_026', 'resource_user_027', 'resource_user_028', 'resource_user_029', 'resource_user_030', 'resource_user_031', 'resource_user_032', 'resource_user_033', 'resource_user_034', 'resource_user_035', 'resource_user_036', 'resource_user_037', 'resource_user_038', 'resource_user_039', 'resource_user_040', 'resource_user_041', 'resource_user_042', 'resource_user_043', 'resource_user_044', 'resource_user_045', 'resource_user_046', 'resource_user_047', 'resource_user_048', 'resource_user_049', 'resource_user_050', 'resource_user_051', 'resource_user_052', 'resource_user_053', 'resource_user_054', 'resource_user_056', 'resource_user_057', 'resource_user_058', 'resource_user_059', 'resource_user_060', 'resource_user_061', 'resource_user_062', 'resource_user_063', 'resource_user_064', 'resource_user_065', 'resource_user_066', 'resource_user_067', 'resource_user_068', 'resource_user_069', 'resource_user_070', 'resource_user_071', 'resource_user_072', 'resource_user_073', 'resource_user_074', 'resource_user_075', 'resource_user_076', 'resource_user_077', 'resource_user_078', 'resource_user_079', 'resource_user_080', 'resource_user_081', 'resource_user_082', 'resource_user_083', 'resource_user_084', 'resource_user_085', 'resource_user_086', 'resource_user_087', 'resource_user_088', 'resource_user_089', 'resource_user_090', 'resource_user_091', 'resource_user_092', 'resource_user_093', 'resource_user_094', 'resource_user_095', 'resource_user_096', 'resource_user_097', 'resource_user_098', 'resource_user_099', 'resource_user_100', 'resource_user_101', 'resource_user_102', 'resource_user_103', 'resource_user_104', 'resource_user_105', 'resource_user_106', 'resource_user_107', 'resource_user_108', 'resource_user_109', 'resource_user_110', 'resource_user_111', 'resource_user_112', 'resource_user_113', 'resource_user_114', 'resource_user_115', 'resource_user_116', 'resource_user_117', 'resource_user_118', 'resource_user_119', 'resource_user_120', 'resource_user_121', 'resource_user_122', 'resource_user_123', 'resource_user_124', 'resource_user_125', 'resource_user_126', 'resource_user_127', 'resource_user_128', 'resource_user_129', 'resource_user_130', 'resource_user_131', 'resource_user_132', 'resource_user_133', 'resource_user_134', 'resource_user_135', 'resource_user_136', 'resource_user_137', 'resource_user_138', 'resource_user_139', 'resource_user_140', 'resource_user_141', 'resource_user_142', 'resource_user_143', 'resource_user_144', 'resource_user_145', 'resource_user_146', 'resource_user_147', 'resource_user_148', 'resource_user_149', 'resource_user_150', 'resource_user_151', 'resource_user_152', 'resource_user_153', 'resource_user_154', 'resource_user_155', 'resource_user_156', 'resource_user_157', 'resource_user_158', 'resource_user_159', 'resource_user_160', 'resource_user_161', 'resource_user_162', 'resource_user_163', 'resource_user_164', 'resource_user_165', 'resource_user_166', 'resource_user_167', 'resource_user_168', 'resource_user_169', 'resource_user_170', 'resource_user_171', 'resource_user_172', 'resource_user_173', 'resource_user_174', 'resource_user_175', 'resource_user_176', 'resource_user_177', 'resource_user_178', 'resource_user_179', 'resource_user_180', 'resource_user_181', 'resource_user_182', 'resource_user_183', 'resource_user_184', 'resource_user_185', 'resource_user_186', 'resource_user_187', 'resource_user_188', 'resource_user_189', 'resource_user_190', 'resource_user_191', 'resource_user_192', 'resource_user_193', 'resource_user_194', 'resource_user_195', 'resource_user_196', 'resource_user_197', 'resource_user_198', 'resource_user_199', 'resource_user_200', 'resource_user_201', 'resource_user_202', 'resource_user_203', 'resource_user_204', 'resource_user_205', 'resource_user_206', 'resource_user_207', 'resource_user_208', 'resource_user_209', 'resource_user_210', 'resource_user_211', 'resource_user_212', 'resource_user_213', 'resource_user_214', 'resource_user_215', 'resource_user_216', 'resource_user_217', 'resource_user_218', 'resource_user_220', 'resource_user_221', 'resource_user_222', 'resource_user_223', 'resource_user_224', 'resource_user_225', 'resource_user_226', 'resource_user_227', 'resource_user_228', 'resource_user_229', 'resource_user_230', 'resource_user_231', 'resource_user_232', 'resource_user_233', 'resource_user_234', 'resource_user_235', 'resource_user_236', 'resource_user_237', 'resource_user_238', 'resource_user_239', 'resource_user_240', 'resource_user_241', 'resource_user_242', 'resource_user_243', 'resource_user_244', 'resource_user_246', 'resource_user_247', 'resource_user_248', 'resource_user_249', 'resource_user_250', 'resource_user_251', 'resource_user_252', 'resource_user_253', 'resource_user_254', 'resource_user_255', 'resource_user_256', 'resource_user_257', 'resource_user_258', 'resource_user_259', 'resource_user_260', 'resource_user_261', 'resource_user_262', 'resource_user_263', 'resource_user_264', 'resource_user_265', 'resource_user_266', 'resource_user_267', 'resource_user_268', 'resource_user_269', 'resource_user_270', 'resource_user_271', 'resource_user_272', 'resource_user_273', 'resource_user_274', 'resource_user_275', 'resource_user_276', 'resource_user_277', 'resource_user_278', 'resource_user_279', 'resource_user_280', 'resource_user_281', 'resource_user_283', 'resource_user_284', 'resource_user_285', 'resource_user_286', 'resource_user_287', 'resource_user_288', 'resource_user_289', 'resource_user_290', 'resource_user_291', 'resource_user_292', 'resource_user_293', 'resource_user_294', 'resource_user_295', 'resource_user_296', 'resource_user_297', 'resource_user_299', 'resource_user_300', 'resource_user_301', 'resource_user_302', 'resource_user_303', 'resource_user_304', 'resource_user_305', 'resource_user_306', 'resource_user_307', 'resource_user_308', 'resource_user_309', 'resource_user_310', 'resource_user_311', 'resource_user_312', 'resource_user_313', 'resource_user_314', 'resource_user_315', 'resource_user_316', 'resource_user_317', 'resource_user_318', 'resource_user_319', 'resource_user_320', 'resource_user_321', 'resource_user_322', 'resource_user_323', 'resource_user_324', 'resource_user_325', 'resource_user_326', 'resource_user_327', 'resource_user_328', 'resource_user_329', 'resource_user_330', 'resource_user_331', 'resource_user_332', 'resource_user_333', 'resource_user_335', 'resource_user_336', 'resource_user_337', 'resource_user_338', 'resource_user_342', 'resource_user_344', 'resource_user_345', 'resource_user_346', 'resource_user_347', 'resource_user_348', 'resource_user_349', 'resource_user_350', 'resource_user_351', 'resource_user_352', 'resource_user_353', 'resource_user_354', 'resource_user_355', 'resource_user_356', 'resource_user_357', 'resource_user_358', 'resource_user_359', 'resource_user_360', 'resource_user_361', 'resource_user_362', 'resource_user_363', 'resource_user_364', 'resource_user_365', 'resource_user_366', 'resource_user_367', 'resource_user_368', 'resource_user_369', 'resource_user_370', 'resource_user_371', 'resource_user_372', 'resource_user_373', 'resource_user_374', 'resource_user_375', 'resource_user_376', 'resource_user_377', 'resource_user_378', 'resource_user_379', 'resource_user_380', 'resource_user_381', 'resource_user_382', 'resource_user_383', 'resource_user_384', 'resource_user_385', 'resource_user_386', 'resource_user_387', 'resource_user_388', 'resource_user_389', 'resource_user_390', 'resource_user_391', 'resource_user_392', 'resource_user_393', 'resource_user_394', 'resource_user_395', 'resource_user_396', 'resource_user_398', 'resource_user_399', 'resource_user_400', 'resource_user_401', 'resource_user_402', 'resource_user_403', 'resource_user_404', 'resource_user_405', 'resource_user_407', 'resource_user_410', 'resource_user_411', 'resource_user_412', 'resource_user_413', 'resource_user_416', 'resource_user_417', 'resource_user_419', 'resource_user_421', 'resource_user_423', 'resource_user_424', 'resource_user_427', 'resource_user_428', 'resource_user_429', 'resource_user_430', 'resource_user_431', 'resource_user_432', 'resource_user_433', 'resource_user_434', 'resource_user_435', 'resource_user_436', 'resource_user_437', 'resource_user_438', 'resource_user_439', 'resource_user_440', 'resource_user_441', 'resource_user_442', 'resource_user_444', 'resource_user_445', 'resource_user_446', 'resource_user_447', 'resource_user_448', 'resource_user_449', 'resource_user_450', 'resource_user_451', 'resource_user_452', 'resource_user_453', 'resource_user_454', 'resource_user_455', 'resource_user_456', 'resource_user_457', 'resource_user_458', 'resource_user_459', 'resource_user_460', 'resource_user_461', 'resource_user_462', 'resource_user_463', 'resource_user_464', 'resource_user_465', 'resource_user_466', 'resource_user_467', 'resource_user_468', 'resource_user_469', 'resource_user_470', 'resource_user_471', 'resource_user_472', 'resource_user_473', 'resource_user_474', 'resource_user_475', 'resource_user_476', 'resource_user_477', 'resource_user_478', 'resource_user_479', 'resource_user_480', 'resource_user_481', 'resource_user_482', 'resource_user_483', 'resource_user_484', 'resource_user_485', 'resource_user_486', 'resource_user_487', 'resource_user_488', 'resource_user_490', 'resource_user_491', 'resource_user_492', 'resource_user_493', 'resource_user_494', 'resource_user_495', 'resource_user_496', 'resource_user_497', 'resource_user_498', 'resource_user_499', 'resource_user_500', 'resource_user_501', 'resource_user_502', 'resource_user_503', 'resource_user_504', 'resource_user_505', 'resource_user_506', 'resource_user_507', 'resource_user_508', 'resource_user_509', 'resource_user_510', 'resource_user_511', 'resource_user_512', 'resource_user_513', 'resource_user_514', 'resource_user_515', 'resource_user_516', 'resource_user_517', 'resource_user_518', 'resource_user_519', 'resource_user_520', 'resource_user_521', 'resource_user_522', 'resource_user_523', 'resource_user_524', 'resource_user_525', 'resource_user_526', 'resource_user_527', 'resource_user_528', 'resource_user_529', 'resource_user_530', 'resource_user_531', 'resource_user_532', 'resource_user_533', 'resource_user_535', 'resource_user_536', 'resource_user_537', 'resource_user_538', 'resource_user_539', 'resource_user_541', 'resource_user_543', 'resource_user_544', 'resource_user_545', 'resource_user_546', 'resource_user_547', 'resource_user_548', 'resource_user_549', 'resource_user_550', 'resource_user_551', 'resource_user_552', 'resource_user_554', 'resource_user_555', 'resource_user_556', 'resource_user_557', 'resource_user_558', 'resource_user_559', 'resource_user_561', 'resource_user_563', 'resource_user_564', 'resource_user_565', 'resource_user_566', 'resource_user_567', 'resource_user_568', 'resource_user_569', 'resource_user_570', 'resource_user_571', 'resource_user_572', 'resource_user_573', 'resource_user_574', 'resource_user_576', 'resource_user_577', 'resource_user_578', 'resource_user_579', 'resource_user_580', 'resource_user_581', 'resource_user_583', 'resource_user_585', 'resource_user_588', 'resource_user_589', 'resource_user_591', 'resource_user_592', 'resource_user_597', 'resource_user_598', 'resource_user_600', 'resource_user_601', 'resource_user_602', 'resource_user_603', 'resource_user_604', 'resource_user_605']\n"
     ]
    }
   ],
   "source": [
    "df_2019['next_activity'] = df_2019.groupby('org:resource')['concept:name'].shift(-1)\n",
    "df_2019 = df_2019.dropna(subset=['next_activity'])\n",
    "\n",
    "# Encoding the 'concept:name' column, as these are the activities\n",
    "column_to_encode = 'concept:name'\n",
    "\n",
    "# OHE BPIC 2017 using pandas' get_dummies\n",
    "df_2019_OHE = pd.get_dummies(df_2019, columns=[column_to_encode], prefix='activity')\n",
    "df_2019_OHE = pd.get_dummies(df_2019_OHE, columns=['org:resource'], prefix='resource')\n",
    "\n",
    "print(\"Columns in final dataframe 2019:\")\n",
    "print(df_2019_OHE.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbae5fb",
   "metadata": {},
   "source": [
    "<h1> 2-Gram encoding the event-logs </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc983d4",
   "metadata": {},
   "source": [
    "<h4> Creating bigram features function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3efae075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bigram_features(df, window_size=5):\n",
    "    \"\"\"\n",
    "    Splits resource streams into sliding windows and computes\n",
    "    traditional 2-gram (Bigram) counts for each window.\n",
    "    \"\"\"\n",
    "    print(f\"Generating 2-gram features with window size: {window_size})\")\n",
    "    \n",
    "    feature_dicts = []\n",
    "    targets = []    \n",
    "    \n",
    "    for resource, group in df.groupby('org:resource'):\n",
    "        activities = group['activity'].values\n",
    "        \n",
    "        # Sliding window over the resource's timeline\n",
    "        for i in range(len(activities) - window_size):\n",
    "            # The context (past N events)\n",
    "            window = activities[i : i + window_size]\n",
    "            # The target (next event)\n",
    "            next_act = activities[i + window_size]\n",
    "            \n",
    "            bigrams = [\n",
    "                f\"{window[j]}->{window[j+1]}\" \n",
    "                for j in range(len(window) - 1)\n",
    "            ]\n",
    "\n",
    "            bigram_counts = dict(Counter(bigrams))\n",
    "            \n",
    "            feature_dicts.append(bigram_counts)\n",
    "            targets.append(next_act)\n",
    "\n",
    "    return feature_dicts, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a6958a",
   "metadata": {},
   "source": [
    "<h4> 2-Gram encoding the 2013 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e9a23e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-gram features with window size: 5)\n"
     ]
    }
   ],
   "source": [
    "df_2013_2gram = event_log_2013[[\n",
    "    'case:concept:name', \n",
    "    'concept:name', \n",
    "    'org:resource', \n",
    "    'time:timestamp', \n",
    "]].copy()\n",
    "\n",
    "df_2013_2gram['activity'] = df_2013_2gram['concept:name']\n",
    "\n",
    "df_2013_2gram = df_2013_2gram.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "df_2013_2gram = df_2013_2gram.dropna(subset=['org:resource'])\n",
    "\n",
    "feature_dicts_2013, y_2gram_2013 = generate_bigram_features(df_2013_2gram)\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_2gram_2013 = vectorizer.fit_transform(feature_dicts_2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88bcce",
   "metadata": {},
   "source": [
    "<h4> 2-Gram encoding the 2017 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2e6f52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-gram features with window size: 5)\n"
     ]
    }
   ],
   "source": [
    "df_2017_2gram = event_log_2017[[\n",
    "    'case:concept:name', \n",
    "    'concept:name', \n",
    "    'org:resource', \n",
    "    'time:timestamp', \n",
    "]].copy()\n",
    "\n",
    "df_2017_2gram['activity'] = df_2017_2gram['concept:name']\n",
    "\n",
    "df_2017_2gram = df_2017_2gram.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "df_2017_2gram = df_2017_2gram.dropna(subset=['org:resource'])\n",
    "\n",
    "# Run it\n",
    "feature_dicts_2017, y_2gram_2017 = generate_bigram_features(df_2017_2gram)\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_2gram_2017 = vectorizer.fit_transform(feature_dicts_2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f5deb",
   "metadata": {},
   "source": [
    "<h4> 2-Gram encoding the 2018 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11c904e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Generating 2-gram features (Window Size: 5)...\n"
     ]
    }
   ],
   "source": [
    "df_2018_2gram = event_log_2018[[\n",
    "    'case:concept:name', \n",
    "    'concept:name', \n",
    "    'org:resource', \n",
    "    'time:timestamp', \n",
    "]].copy()\n",
    "\n",
    "df_2018_2gram['activity'] = df_2018_2gram['concept:name']\n",
    "\n",
    "df_2018_2gram = df_2018_2gram.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "df_2018_2gram = df_2018_2gram.dropna(subset=['org:resource'])\n",
    "\n",
    "# Run it\n",
    "feature_dicts_2018, y_2gram_2018 = generate_bigram_features(df_2018_2gram)\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_2gram_2018 = vectorizer.fit_transform(feature_dicts_2018)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd02bb8",
   "metadata": {},
   "source": [
    "<h4> 2-Gram encoding the 2019 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e755eb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 2-gram features with window size: 5)\n"
     ]
    }
   ],
   "source": [
    "df_2019_2gram = event_log_2019[[\n",
    "    'case:concept:name', \n",
    "    'concept:name', \n",
    "    'org:resource', \n",
    "    'time:timestamp', \n",
    "]].copy()\n",
    "\n",
    "df_2019_2gram['activity'] = df_2019_2gram['concept:name']\n",
    "\n",
    "df_2019_2gram = df_2019_2gram.sort_values(by=['org:resource', 'time:timestamp'])\n",
    "\n",
    "df_2019_2gram = df_2019_2gram.dropna(subset=['org:resource'])\n",
    "\n",
    "# Run it\n",
    "feature_dicts_2019, y_2gram_2019 = generate_bigram_features(df_2019_2gram)\n",
    "\n",
    "# Vectorize\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_2gram_2019 = vectorizer.fit_transform(feature_dicts_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5167474",
   "metadata": {},
   "source": [
    "<h1> Training Random Forest model on OHE event-logs </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad50b67",
   "metadata": {},
   "source": [
    "<h4> Random Forest Training Pipeline </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58fe3bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_rf(X, y, random_state=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100, random_state=random_state, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "\n",
    "def add_next_activity(df):\n",
    "\n",
    "    resource_sequences = df.groupby('resource')['prefix'].apply(lambda x: x.iloc[-1]).to_dict()\n",
    "    next_activities = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        prefix = row['prefix']\n",
    "        resource = row['resource']\n",
    "        full_seq = resource_sequences[resource]\n",
    "\n",
    "        if len(prefix) < len(full_seq):\n",
    "            next_activities.append(full_seq[len(prefix)])  # next activity\n",
    "        else:\n",
    "            next_activities.append(None)  # last prefix has no next activity\n",
    "    \n",
    "    df['next_activity'] = next_activities\n",
    "    df = df.dropna(subset=['next_activity'])\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990fa75d",
   "metadata": {},
   "source": [
    "<h4> Training Random forest on 2013 incidents log </h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d375edda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "Accepted     36157\n",
      "Completed    13490\n",
      "Queued       10084\n",
      "Unmatched        1\n",
      "Name: count, dtype: int64\n",
      "Got to here\n"
     ]
    }
   ],
   "source": [
    "# BPIC 2013 incidents\n",
    "feature_columns_2013 = [col for col in df_2013_OHE.columns if col.startswith('activity_') or col.startswith('resource')]\n",
    "X_2013_OHE = df_2013_OHE[feature_columns_2013]\n",
    "y_2013_OHE = df_2013_OHE['next_activity']\n",
    "\n",
    "accuracy_2013_OHE, f1_2013_OHE = train_evaluate_rf(X_2013_OHE, y_2013_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28985d87",
   "metadata": {},
   "source": [
    "<h4> Training Random Forest on 2017 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPIC 2017\n",
    "feature_columns_2017 = [col for col in df_2017_OHE.columns if col.startswith('activity_') or col.startswith('resource')]\n",
    "X_2017_OHE = df_2017_OHE[feature_columns_2017]\n",
    "y_2017_OHE = df_2017_OHE['next_activity']\n",
    "\n",
    "accuracy_2017_OHE_OHE, f1_2017_OHE = train_evaluate_rf(X_2017_OHE, y_2017_OHE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e83d711",
   "metadata": {},
   "source": [
    "<h4>Training Random Forest on 2018 event-log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869639f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPIC 2018\n",
    "feature_columns_2018 = [col for col in df_2018_OHE.columns if col.startswith('activity_') or col.startswith('resource')]\n",
    "X_2018_OHE = df_2018_OHE[feature_columns_2018]\n",
    "y_2018_OHE = df_2018_OHE['next_activity']\n",
    "\n",
    "accuracy_2018_OHE_OHE, f1_2018_OHE = train_evaluate_rf(X_2018_OHE, y_2018_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d0998b",
   "metadata": {},
   "source": [
    "<h4> Training Random Forest on 2019 event-log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515e86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPIC 2019\n",
    "feature_columns_2019 = [col for col in df_2019_OHE.columns if col.startswith('activity_') or col.startswith('resource')]\n",
    "X_2019_OHE = df_2019_OHE[feature_columns_2019]\n",
    "y_2019_OHE = df_2019_OHE['next_activity']\n",
    "\n",
    "accuracy_2019_OHE_OHE, f1_2019_OHE = train_evaluate_rf(X_2019_OHE, y_2019_OHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0130f3f",
   "metadata": {},
   "source": [
    "<h1> Random Forest Classifier Results </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4cab45",
   "metadata": {},
   "source": [
    "<h4> One-Hot Encoding results </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724f15e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 2013: 0.7071534441064046\n",
      "F1-score 2013: 0.6681529055831646\n",
      "Accuracy 2017: 0.6663145110305128\n",
      "F1-score 2017: 0.6437193163447493\n",
      "Accuracy 2018: 0.6308069869794619\n",
      "F1-score 2018: 0.6211972047571935\n",
      "Accuracy 2019: 0.9579513506906246\n",
      "F1-score 2019: 0.9543415517430779\n",
      "Average accuracy is: 0.740556573201751\n",
      "Average f1-score is: 0.7218527446070464\n"
     ]
    }
   ],
   "source": [
    "# Results OHE 2013 log\n",
    "print(f\"Accuracy 2013: {accuracy_2013_OHE_OHE}\")\n",
    "print(f\"F1-score 2013: {f1_2013_OHE}\")\n",
    "\n",
    "# Results OHE 2017 log\n",
    "print(f\"Accuracy 2017: {accuracy_2017_OHE_OHE}\")\n",
    "print(f\"F1-score 2017: {f1_2017_OHE}\")\n",
    "\n",
    "\n",
    "# Results OHE 2018 log\n",
    "print(f\"Accuracy 2018: {accuracy_2018_OHE_OHE}\")\n",
    "print(f\"F1-score 2018: {f1_2018_OHE}\")\n",
    "\n",
    "# Results OHE 2019 log\n",
    "print(f\"Accuracy 2019: {accuracy_2019_OHE_OHE}\")\n",
    "print(f\"F1-score 2019: {f1_2019_OHE}\")\n",
    "\n",
    "\n",
    "# Aggregated results \n",
    "aggregated_accuracy = statistics.mean([accuracy_2013_OHE_OHE,accuracy_2017_OHE_OHE, accuracy_2018_OHE_OHE, accuracy_2019_OHE_OHE])\n",
    "print(f\"Average accuracy is: {aggregated_accuracy}\")\n",
    "aggregated_f1score = statistics.mean([f1_2013_OHE,f1_2017_OHE,f1_2018_OHE,f1_2019_OHE])\n",
    "print(f\"Average f1-score is: {aggregated_f1score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2561e5",
   "metadata": {},
   "source": [
    "<h1> Training Random Forest on 2-gram encoding </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17880299",
   "metadata": {},
   "source": [
    "<h4> Training Random Forest on 2-gram encoding on 2013 incidents log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9491ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution:\n",
      "Accepted     36157\n",
      "Completed    13490\n",
      "Queued       10084\n",
      "Unmatched        1\n",
      "Name: count, dtype: int64\n",
      "Got to here\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGot to here\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m y_pred = rf.predict(X_test)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m accuracy_2013_2gram, f1_2013_gram = \u001b[43mtrain_evaluate_rf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_2gram_2013\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_2gram_2013\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mtrain_evaluate_rf\u001b[39m\u001b[34m(X, y, random_state)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_evaluate_rf\u001b[39m(X, y, random_state=\u001b[32m1\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     rf = RandomForestClassifier(n_estimators=\u001b[32m100\u001b[39m, random_state=random_state, n_jobs=-\u001b[32m1\u001b[39m)\n\u001b[32m      7\u001b[39m     rf.fit(X_train, y_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UU Jaar 4/OZP/Resource-Centric-NAP/.venv/lib/python3.14/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UU Jaar 4/OZP/Resource-Centric-NAP/.venv/lib/python3.14/site-packages/sklearn/model_selection/_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UU Jaar 4/OZP/Resource-Centric-NAP/.venv/lib/python3.14/site-packages/sklearn/model_selection/_split.py:1927\u001b[39m, in \u001b[36mBaseShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   1897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[32m   1898\u001b[39m \n\u001b[32m   1899\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1924\u001b[39m \u001b[33;03mto an integer.\u001b[39;00m\n\u001b[32m   1925\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1926\u001b[39m X, y, groups = indexable(X, y, groups)\n\u001b[32m-> \u001b[39m\u001b[32m1927\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_iter_indices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/UU Jaar 4/OZP/Resource-Centric-NAP/.venv/lib/python3.14/site-packages/sklearn/model_selection/_split.py:2342\u001b[39m, in \u001b[36mStratifiedShuffleSplit._iter_indices\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2340\u001b[39m class_counts = np.bincount(y_indices)\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.min(class_counts) < \u001b[32m2\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2343\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe least populated class in y has only 1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2344\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m member, which is too few. The minimum\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2345\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m number of groups for any class cannot\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m be less than 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2347\u001b[39m     )\n\u001b[32m   2349\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_train < n_classes:\n\u001b[32m   2350\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2351\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe train_size = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m should be greater or \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2352\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mequal to the number of classes = \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m % (n_train, n_classes)\n\u001b[32m   2353\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2gram_2013, y_2gram_2013, test_size=0.2, random_state=1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_2013_2gram, f1_2013_gram = train_evaluate_rf(X_2gram_2013, y_2gram_2013)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03f8bd",
   "metadata": {},
   "source": [
    "<h4> Training Random Forest on 2-gram encoding on 2017 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc0e19a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8605\n",
      "F1 score: 0.8501345416297915\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2gram_2017, y_2gram_2017, test_size=0.2, random_state=1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_2gram_2017 = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_2gram_2017:.4f}\")\n",
    "f1score_2gram_2017 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 score: {f1score_2gram_2017}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90293d70",
   "metadata": {},
   "source": [
    "<h4> Training Random Forest on 2-gram encoding on 2018 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a23743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2gram_2018, y_2gram_2018, test_size=0.2, random_state=1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_2gram_2018 = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_2gram_2018:.4f}\")\n",
    "f1score_2gram_2018 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 score: {f1score_2gram_2018}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0842396",
   "metadata": {},
   "source": [
    "<h4> Training Random Forest on 2-gram encoding on 2019 log </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da08ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9568\n",
      "F1 score: 0.9526134041550297\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_2gram_2019, y_2gram_2019, test_size=0.2, random_state=1)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "accuracy_2gram_2019 = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy_2gram_2019:.4f}\")\n",
    "f1score_2gram_2019 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"F1 score: {f1score_2gram_2019}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e91c64",
   "metadata": {},
   "source": [
    "<h4> Aggregating Random Forest with 2-gram results </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b0e146b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy is: 0.8384522867796331\n",
      "Average f1-score is: 0.8114549707865334\n"
     ]
    }
   ],
   "source": [
    "# Aggregated results \n",
    "aggregated_accuracy = statistics.mean([accuracy_2gram_2013, accuracy_2gram_2017, accuracy_2gram_2019])\n",
    "print(f\"Average accuracy is: {aggregated_accuracy}\")\n",
    "aggregated_f1score = statistics.mean([f1score_2gram_2013, f1score_2gram_2017, f1score_2gram_2019])\n",
    "print(f\"Average f1-score is: {aggregated_f1score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
